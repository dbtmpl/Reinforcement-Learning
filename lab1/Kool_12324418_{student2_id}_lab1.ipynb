{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parts of this assignment will be **automatically graded**. Please take note of the following:\n",
    "- Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "- You can add additional cells, but it is not recommended to (re)move cells. Cells required for autograding cannot be moved and cells containing tests cannot be edited.\n",
    "- You are allowed to use a service such as [Google Colaboratory](https://colab.research.google.com/) to work together. However, you **cannot** hand in the notebook that was hosted on Google Colaboratory, but you need to copy your answers into the original notebook and verify that it runs succesfully offline. This is because Google Colaboratory destroys the metadata required for grading.\n",
    "- Name your notebook **exactly** `{TA_name}_{student1_id}_{student2_id}_lab{i}.ipynb`, for example `wouter_12345_67890_lab1.ipynb` (or tim|elise|david|qi, depending on your TA), **otherwise your submission will be skipped by our regex and you will get 0 points** (but no penalty as we cannot parse your student ids ;)).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your names below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joaq\n",
    "NAMES = \"David Biertimpel, Joa\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4eed621d3748a44866956caa0de5247b",
     "grade": false,
     "grade_id": "cell-fc69f22067705372",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from tqdm import tqdm as _tqdm\n",
    "\n",
    "def tqdm(*args, **kwargs):\n",
    "    return _tqdm(*args, **kwargs, mininterval=1)  # Safety, do not overflow buffer\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bab7b3976d6730a0739fd462766b1d42",
     "grade": false,
     "grade_id": "cell-9ebb0d5b306dbdea",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 1. Policy Evaluation (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8d010aef9b5b288e694006a2aefe67e0",
     "grade": false,
     "grade_id": "cell-1078e8f0b90517ce",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In this exercise we will evaluate a policy, e.g. find the value function for a policy. The problem we consider is the gridworld from Example 4.1 in the book. The environment is implemented as `GridworldEnv`, which is a subclass of the `Env` class from [OpenAI Gym](https://github.com/openai/gym). This means that we can interact with the environment. We can look at the documentation to see how we can interact with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "990081b68602e7e0c46f2edeab0fcb53",
     "grade": false,
     "grade_id": "cell-de586c5ac92d8d74",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from gridworld import GridworldEnv\n",
    "env = GridworldEnv()\n",
    "# Lets see what this is\n",
    "?env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "814f4db75653991276d29ebff9d6ae37",
     "grade": false,
     "grade_id": "cell-b3a84dfb0e66a0c0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# To have a quick look into the code\n",
    "??env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fe850a3b9a1be42ae79b895d206ac3b6",
     "grade": false,
     "grade_id": "cell-b2162d776f0c2014",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now we want to evaluate a policy by using Dynamic Programming. For more information, see the [Intro to RL](https://drive.google.com/open?id=1opPSz5AZ_kVa1uWOdOiveNiBFiEOHjkG) book, section 4.1. This algorithm requires knowledge of the problem dynamics in the form of the transition probabilities $p(s',r|s,a)$. In general these are not available, but for our gridworld we know the dynamics and these can be accessed as `env.P`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: [(1.0, 0, 0.0, True)],\n",
       "  1: [(1.0, 0, 0.0, True)],\n",
       "  2: [(1.0, 0, 0.0, True)],\n",
       "  3: [(1.0, 0, 0.0, True)]},\n",
       " 1: {0: [(1.0, 1, -1.0, False)],\n",
       "  1: [(1.0, 2, -1.0, False)],\n",
       "  2: [(1.0, 5, -1.0, False)],\n",
       "  3: [(1.0, 0, -1.0, True)]},\n",
       " 2: {0: [(1.0, 2, -1.0, False)],\n",
       "  1: [(1.0, 3, -1.0, False)],\n",
       "  2: [(1.0, 6, -1.0, False)],\n",
       "  3: [(1.0, 1, -1.0, False)]},\n",
       " 3: {0: [(1.0, 3, -1.0, False)],\n",
       "  1: [(1.0, 3, -1.0, False)],\n",
       "  2: [(1.0, 7, -1.0, False)],\n",
       "  3: [(1.0, 2, -1.0, False)]},\n",
       " 4: {0: [(1.0, 0, -1.0, True)],\n",
       "  1: [(1.0, 5, -1.0, False)],\n",
       "  2: [(1.0, 8, -1.0, False)],\n",
       "  3: [(1.0, 4, -1.0, False)]},\n",
       " 5: {0: [(1.0, 1, -1.0, False)],\n",
       "  1: [(1.0, 6, -1.0, False)],\n",
       "  2: [(1.0, 9, -1.0, False)],\n",
       "  3: [(1.0, 4, -1.0, False)]},\n",
       " 6: {0: [(1.0, 2, -1.0, False)],\n",
       "  1: [(1.0, 7, -1.0, False)],\n",
       "  2: [(1.0, 10, -1.0, False)],\n",
       "  3: [(1.0, 5, -1.0, False)]},\n",
       " 7: {0: [(1.0, 3, -1.0, False)],\n",
       "  1: [(1.0, 7, -1.0, False)],\n",
       "  2: [(1.0, 11, -1.0, False)],\n",
       "  3: [(1.0, 6, -1.0, False)]},\n",
       " 8: {0: [(1.0, 4, -1.0, False)],\n",
       "  1: [(1.0, 9, -1.0, False)],\n",
       "  2: [(1.0, 12, -1.0, False)],\n",
       "  3: [(1.0, 8, -1.0, False)]},\n",
       " 9: {0: [(1.0, 5, -1.0, False)],\n",
       "  1: [(1.0, 10, -1.0, False)],\n",
       "  2: [(1.0, 13, -1.0, False)],\n",
       "  3: [(1.0, 8, -1.0, False)]},\n",
       " 10: {0: [(1.0, 6, -1.0, False)],\n",
       "  1: [(1.0, 11, -1.0, False)],\n",
       "  2: [(1.0, 14, -1.0, False)],\n",
       "  3: [(1.0, 9, -1.0, False)]},\n",
       " 11: {0: [(1.0, 7, -1.0, False)],\n",
       "  1: [(1.0, 11, -1.0, False)],\n",
       "  2: [(1.0, 15, -1.0, True)],\n",
       "  3: [(1.0, 10, -1.0, False)]},\n",
       " 12: {0: [(1.0, 8, -1.0, False)],\n",
       "  1: [(1.0, 13, -1.0, False)],\n",
       "  2: [(1.0, 12, -1.0, False)],\n",
       "  3: [(1.0, 12, -1.0, False)]},\n",
       " 13: {0: [(1.0, 9, -1.0, False)],\n",
       "  1: [(1.0, 14, -1.0, False)],\n",
       "  2: [(1.0, 13, -1.0, False)],\n",
       "  3: [(1.0, 12, -1.0, False)]},\n",
       " 14: {0: [(1.0, 10, -1.0, False)],\n",
       "  1: [(1.0, 15, -1.0, True)],\n",
       "  2: [(1.0, 14, -1.0, False)],\n",
       "  3: [(1.0, 13, -1.0, False)]},\n",
       " 15: {0: [(1.0, 15, 0.0, True)],\n",
       "  1: [(1.0, 15, 0.0, True)],\n",
       "  2: [(1.0, 15, 0.0, True)],\n",
       "  3: [(1.0, 15, 0.0, True)]}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a moment to figure out what P represents. \n",
    "# Note that this is a deterministic environment. \n",
    "# What would a stochastic environment look like?\n",
    "env.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d2d2b829d45d264cf8a6194dc8ccc132",
     "grade": false,
     "grade_id": "cell-209a484040bd874f",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def policy_eval(policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    \n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "    \"\"\"\n",
    "    # Start with a random (all 0) value function\n",
    "    V = np.zeros(env.nS) \n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        V_old = V.copy()\n",
    "        for s in np.arange(env.nS):\n",
    "            v = V_old[s]\n",
    "            state_pol = policy[s]\n",
    "            state_dyn = env.P[s]\n",
    "            Vs_new = 0\n",
    "            for a, pi_a in enumerate(state_pol):\n",
    "                current_dynamics = state_dyn[a]\n",
    "                update = sum(psrsa * (reward + discount_factor * V_old[succ_state]) for psrsa, succ_state, reward, _ in current_dynamics)\n",
    "                Vs_new +=  pi_a * update\n",
    "            V[s] = Vs_new\n",
    "            delta = np.max([delta, np.absolute(v - Vs_new)])\n",
    "            \n",
    "        if delta < theta:\n",
    "            break\n",
    "        \n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.        , -13.99989315, -19.99984167, -21.99982282,\n",
       "       -13.99989315, -17.99986052, -19.99984273, -19.99984167,\n",
       "       -19.99984167, -19.99984273, -17.99986052, -13.99989315,\n",
       "       -21.99982282, -19.99984167, -13.99989315,   0.        ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's run your code, does it make sense?\n",
    "random_policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "V = policy_eval(random_policy, env)\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_gridworld_value(V):\n",
    "    plt.figure()\n",
    "    c = plt.pcolormesh(V, cmap='gray')\n",
    "    plt.colorbar(c)\n",
    "    plt.gca().invert_yaxis()  # In the array, first row = 0 is on top\n",
    "\n",
    "# Making a plot always helps\n",
    "plot_gridworld_value(V.reshape(env.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5d879d65fc89af254883e1b68234e76e",
     "grade": true,
     "grade_id": "cell-b5c9d69b1731aff5",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Test: When you hand in the nodebook we will check that the value function is (approximately) what we expected\n",
    "# but we need to make sure it is at least of the correct shape\n",
    "v = policy_eval(random_policy, env)\n",
    "assert v.shape == (env.nS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "863ed58baecbbb4923162f40084e870d",
     "grade": false,
     "grade_id": "cell-b680e98c9ff204b8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 2. Policy Iteration (2 points)\n",
    "Using the policy evaluation algorithm we can implement policy iteration to find a good policy for this problem. Note that we do not need to use a discount_factor for episodic tasks but make sure your implementation can handle this correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "cfa494b2b437f9007f6b29b1ed5e0f78",
     "grade": false,
     "grade_id": "cell-383c54749617512c",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def update_policy(policy, env, V, discount_factor):\n",
    "    policy_stable = True\n",
    "    \n",
    "    for s in np.arange(env.nS):\n",
    "            \n",
    "        state_dyn = env.P[s]\n",
    "        old_action = np.argmax(policy[s])\n",
    "\n",
    "        q_values = []\n",
    "        for a in np.arange(env.nA):\n",
    "            current_dynamics = state_dyn[a]\n",
    "            v_value = sum(psrsa * (reward + discount_factor * V[succ_state]) for psrsa, succ_state, reward, _ in current_dynamics)\n",
    "            q_values.append(v_value)\n",
    "\n",
    "        pi_s = np.argmax(q_values)\n",
    "\n",
    "        new_policy = np.zeros(env.nA)\n",
    "        new_policy[pi_s] = 1\n",
    "        policy[s] = new_policy\n",
    "        \n",
    "        if old_action != pi_s:\n",
    "            policy_stable = False\n",
    "\n",
    "    return policy, old_action, policy_stable\n",
    "    \n",
    "\n",
    "def policy_improvement(env, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Policy Improvement Algorithm. Iteratively evaluates and improves a policy\n",
    "    until an optimal policy is found.\n",
    "    \n",
    "    Args:\n",
    "        env: The OpenAI envrionment.\n",
    "        policy_eval_fn: Policy Evaluation function that takes 3 arguments:\n",
    "            policy, env, discount_factor.\n",
    "        discount_factor: gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V). \n",
    "        policy is the optimal policy, a matrix of shape [S, A] where each state s\n",
    "        contains a valid probability distribution over actions.\n",
    "        V is the value function for the optimal policy.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Start with a random policy\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    \n",
    "    while True:\n",
    "        V = policy_eval(policy, env, discount_factor)\n",
    "                \n",
    "        policy, old_action, policy_stable = update_policy(policy, env, V, discount_factor)\n",
    "        \n",
    "        if policy_stable:\n",
    "            return policy, V\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c4ab9c8d01a5902c276a3fbfbcc89e01",
     "grade": true,
     "grade_id": "cell-8c62e92d1f34720b",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Probability Distribution:\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "\n",
      "Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\n",
      "[[0 3 3 2]\n",
      " [0 0 0 2]\n",
      " [0 0 1 2]\n",
      " [0 1 1 0]]\n",
      "^<<v\n",
      "^^^v\n",
      "^^>v\n",
      "^>>^\n",
      "\n",
      "Value Function:\n",
      "[ 0. -1. -2. -3. -1. -2. -3. -2. -2. -3. -2. -1. -3. -2. -1.  0.]\n",
      "\n",
      "Reshaped Grid Value Function:\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWwAAAD8CAYAAABTjp5OAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAYaElEQVR4nO3df4ydVZ3H8ffHUkARRGi1hVaL0ugiy89JgZDsolQtXbZdlW6KWQGVNLoSMWuygk0gYjbBmOhGIdZRiFVRoChSoQitaNBsKEzZUqgFHQkuYxtLobYSoKT0u3/cZ+jl9t65Z2ae5849cz+v5Ibnx5lzv/Po/c7pueeHIgIzM+t+r5voAMzMLI0TtplZJpywzcwy4YRtZpYJJ2wzs0w4YZuZZSIpYUtaIOkJSYOSrmhy/xBJtxT310uaU3agZmbdYqJyYtuELWkKcD1wHnACcKGkExqKfRLYGRHHA18HvlJGcGZm3WYic2JKC3seMBgRT0bEy8DNwOKGMouBlcXxbcC5klRGgGZmXWbCcuJBCWWOBZ6uOx8CzmhVJiL2StoFHA3sqC8kaRmwDOCwww47/d3vfvcYw55cdu7cOdEhdA0/i/38LF5jR0RMH08FCxYsiB07drQvCGzYsGEz8FLdpf6I6C+OS8uJo5WSsJv9VWicz55ShuIX7gfo6+uLgYGBhLef/FatWjXRIXQNP4v9/Cxe40/jrWDHjh2k5hxJL0VEX6vbTa6NKSeOVkqXyBAwu+58FrC1VRlJBwFvAp4bb3BmZmWKiKRXGxOWE1MS9kPAXEnHSToYWAqsbiizGri4OL4AuC+8qpSZdZl9+/YlvdqYsJzYtkuk6H+5DLgHmALcGBGbJV0DDETEauAG4AeSBqn9FVk63sDMzMqU2HpOqWfCcmJKHzYRsQZY03Dtqrrjl4AlZQRkZlaVsv7hP1E5MSlhm5lNBrn31Dphm1nPcMI2M8uEE7aZWQYiImUESFdzwjaznuEWtplZJpywzcwy4YRtZpaBsibOTCQnbDPrGf7S0cwsE25hm5llwF0iZmYZccI2M8uEE7aZWSacsM3MMuCp6WZmGXEL28wsE07YZmaZyD1hp2zCi6QFkp6QNCjpiib3L5H0jKSNxevS8kM1MxufknZNnzBtW9iSpgDXA++ntnX7Q5JWR8TvGoreEhGXVRCjmdm4TYYvHVNa2POAwYh4MiJeBm4GFlcblplZ+XJvYack7GOBp+vOh4prjT4iaZOk2yTNLiU6M7MS9ULCVpNrjb/Rz4E5EXESsA5Y2bQiaZmkAUkDzzzzzOgiNTMbp15I2ENAfYt5FrC1vkBEPBsRe4rT7wCnN6soIvojoi8i+qZPnz6WeM3MxiQ1WeeesB8C5ko6TtLBwFJgdX0BSTPrThcBW8oL0cysHLkn7LajRCJir6TLgHuAKcCNEbFZ0jXAQESsBj4raRGwF3gOuKTCmM3MxiT3USJJE2ciYg2wpuHaVXXHVwJXlhuamVm5urn1nMIzHc2sJ3Squ0PSUcAtwBzgKeBfI2Jnk3KvAI8Wp/8XEYva1Z0009HMbDLoUB/2FcAvI2Iu8MvivJkXI+KU4tU2WYMTtpn1kA4l7MXsH9q8EviX8VY4zAnbzHpGhxL2WyNiW/F+24C3tCh3aDEv5QFJSUndfdhm1hNGuZbINEkDdef9EdE/fCJpHTCjyc8tH0VIb4uIrZLeAdwn6dGI+ONIP+CEbWY9YxSt5x0R0TdCPfNb3ZP0F0kzI2JbMUdle4s6thb/fVLSr4FTgRETtrtEzKxndKhLZDVwcXF8MXBHYwFJb5Z0SHE8DTgbaFwB9QBO2GbWMzqUsK8F3i/pD9SWpb4WQFKfpO8WZf4OGJD0CPAr4No4cMnqA7hLxMx6RifGYUfEs8C5Ta4PAJcWx/8D/P1o63bCNrOeMMovHbuSE7aZ9QxPTTczy4QTtplZJpywzcwy0O1rXadwwjaznuGEbWaWCY8SMTPLhFvYZmYZcB+2mVlGck/YbdcSkXSjpO2SHmtxX5K+IWlQ0iZJp5UfppnZ+OW+a3rK4k/fAxaMcP88YG7xWgZ8a/xhmZmVL/eE3bZLJCLulzRnhCKLge9H7bd8QNKRw2vBjlTvzp07WbVq1aiCnaz8HPbzs9hvyZIlEx1C1yjj/xeTYS2RMpZXPRZ4uu58qLh2AEnLii1xBnbv3l3CW5uZpcu9hV1GwlaTa01/44joj4i+iOg74ogjSnhrM7N0uSfsMkaJDAGz685nAVtLqNfMrFTdnIxTlNHCXg1cVIwWORPY1a7/2sxsIkz6FrakHwPnUNtFeAi4GpgKEBErgDXAQmAQeAH4eFXBmpmN1WT40jFllMiFbe4H8JnSIjIzq0g3t55TeKajmfUMJ2wzs0w4YZuZZaDbv1BM4YRtZj3DCdvMLBOTfpSImdlk4Ra2mVkG3IdtZpYRJ2wzs0zknrDLWEvEzCwLnVhLRNISSZsl7ZPUN0K5BZKeKHbruiKlbidsM+sJw2uJpLzG6THgw8D9rQpImgJcT23HrhOACyWd0K5id4mYWc/oRJdIRGwBkJptFfCqecBgRDxZlL2Z2u5dvxvph5ywzaxnjCJhT5M0UHfeHxH9JYbSbKeuM9r9kBO2mfWMUSTsHRExUv/zOmBGk1vLI+KOhPqTd+qq54RtZj2jrC6RiJg/zirGtFOXE7aZ9YQu28DgIWCupOOAPwNLgY+2+yGPEjGzntGhYX0fKnbnOgu4S9I9xfVjJK0p4tgLXAbcA2wBbo2Ize3qdgvbzHpGh0aJ3A7c3uT6VmrbKQ6fr6G2xWKyti1sSTdK2i7psRb3z5G0S9LG4nXVaAIwM+uUSb8JL/A94Drg+yOU+U1EnF9KRGZmFej2ZJwiZRPe+yXNqT4UM7Nq5Z6wy/rS8SxJj0i6W9J7WhWStEzSgKSB3bt3l/TWZmZpOjQ1vTJlfOn4MPD2iHhe0kLgZ8DcZgWLmUL9AO985zvz/lNnZlmZDF0i425hR8TuiHi+OF4DTJU0bdyRmZmVrBe+dByRpBnAXyIiJM2j9kfg2XFHZmZWsm5OxinaJmxJPwbOobYYyhBwNTAVICJWABcAn5a0F3gRWBq5PxUzm5RyT00po0QubHP/OmrD/szMulaXTU0fE890NLOeMelb2GZmk4UTtplZJpywzcwy4YRtZpaBbh9jncIJ28x6hkeJmJllwi1sM7NMOGGbmWXAfdhmZhlxwjYzy4QTtplZJjxKxMwsA+7DNjPLiBO2mVkmnLDNzDLhhG1mlgFvYGBmlpHcW9htd02XNFvSryRtkbRZ0uVNykjSNyQNStok6bRqwjUzG7tO7JouaUmRK/dJ6huh3FOSHpW0UdJASt0pLey9wOcj4mFJhwMbJK2NiN/VlTkPmFu8zgC+VfzXzKxrdKiF/RjwYeDbCWXfGxE7UitO2YR3G7CtOP6bpC3AsUB9wl4MfL/YLf0BSUdKmln8rJlZV+hEwo6ILQCSSq97VH3YkuYApwLrG24dCzxddz5UXHtNwpa0DFgG8IY3vIFVq1aNLtpJys9hvyVLlkx0CF3Dz2K/Mj4jo+zumNbQTdEfEf3jDqIhJOBeSQF8O6X+5IQt6Y3AT4DPRcTuxtstgnnthVpA/QBHHXVU3r3/ZpadUYwS2RERI/U/rwNmNLm1PCLuSHyPsyNiq6S3AGslPR4R94/0A0kJW9JUasn6poj4aZMiQ8DsuvNZwNbEoM3MOqKsLpGImF9CHVuL/26XdDswDxgxYaeMEhFwA7AlIr7Wothq4KJitMiZwC73X5tZt+nEKJEUkg4rBnEg6TDgA9S+rBxRSgv7bOBjwKOSNhbXvgi8DSAiVgBrgIXAIPAC8PHR/gJmZlXqYDL+EPBNYDpwl6SNEfFBSccA342IhcBbgduLLyYPAn4UEb9oV3fKKJHf0ryPur5MAJ9p+5uYmU2gDo0SuR24vcn1rdQatkTEk8DJo63bMx3NrGfkPtPRCdvMeobXEjEzy4A3MDAzy4gTtplZJpywzcwy4YRtZpYBb2BgZpYRt7DNzDLhhG1mlgknbDOzTDhhm5llwBNnzMwy4lEiZmaZcAvbzCwTTthmZhlwH7aZWUacsM3MMpF7wk7ZhHe2pF9J2iJps6TLm5Q5R9IuSRuL11XVhGtmNnb79u1LenWrlBb2XuDzEfFwscvvBklrI+J3DeV+ExHnlx+imdn49UQfdkRsA7YVx3+TtAU4FmhM2GZmXS33hN22S6SepDnAqcD6JrfPkvSIpLslvafFzy+TNCBpYM+ePaMO1sxsPIZb2e1e3Sr5S0dJbwR+AnwuInY33H4YeHtEPC9pIfAzYG5jHRHRD/QDHHXUUd37VMxsUurmZJwiqYUtaSq1ZH1TRPy08X5E7I6I54vjNcBUSdNKjdTMbByGNzCY1F86ShJwA7AlIr7WoswM4C8REZLmUftD8GypkZqZjVPuLeyULpGzgY8Bj0raWFz7IvA2gIhYAVwAfFrSXuBFYGnk/mTMbNLJPS2ljBL5LaA2Za4DrisrKDOzKuSesEc1SsTMLGedGCUi6auSHpe0SdLtko5sUW6BpCckDUq6IqVuJ2wz6wmpybqEVvha4MSIOAn4PXBlYwFJU4DrgfOAE4ALJZ3QrmInbDPrGZ0YJRIR90bE3uL0AWBWk2LzgMGIeDIiXgZuBha3q9sJ28x6xiha2NOGJ/kVr2VjfMtPAHc3uX4s8HTd+VBxbURerc/MesYoujt2RERfq5uS1gEzmtxaHhF3FGWWU1uL6aZmVTQLr11QTthm1hPKnHYeEfNHui/pYuB84NwWQ5yHgNl157OAre3e1wnbzHpGJ4b1SVoAfAH4x4h4oUWxh4C5ko4D/gwsBT7arm73YZtZz+jQ1PTrgMOBtcX+ACsAJB0jaQ1A8aXkZcA9wBbg1ojY3K5it7DNrCd0aiW+iDi+xfWtwMK68zXAmtHU7YRtZj0j95mOTthm1jOcsM3MMuGEbWaWCSdsM7MMDG9gkDMnbDPrGW5hm5llwgnbzCwTTthmZhno1MSZKrWdmi7pUEkPSnpE0mZJX2pS5hBJtxQ7J6yXNKeKYM3MxqNDGxhUJmUtkT3A+yLiZOAUYIGkMxvKfBLYWUzJ/DrwlXLDNDMbvw6tJVKZtgk7ap4vTqcWr8Y/QYuBlcXxbcC5kkbcuNfMrNNyb2En9WEX+49tAI4Hro+I9Q1FXt09ISL2StoFHA3saKhnGfDqzg2rVq0ae+STyJIlSyY6hK7hZ7Gfn0W5uj0Zp0haXjUiXomIU6gtsj1P0okNRZJ2T4iI/ojoG2knBzOzquTewh7VetgR8Vfg18CChluv7p4g6SDgTcBzJcRnZlaaSZ+wJU2XdGRx/HpgPvB4Q7HVwMXF8QXAfS22xTEzmzC5f+mY0oc9E1hZ9GO/jtrOCHdKugYYiIjVwA3ADyQNUmtZL60sYjOzMej21nOKtgk7IjYBpza5flXd8UuAvyExs6426RO2mdlk4YRtZpYJJ2wzs0w4YZuZZcAbGJiZZcQtbDOzTDhhm5llwgnbzCwDPTFxxsxssnDCNjPLRCdGiUj6KvDPwMvAH4GPFwvnNZZ7Cvgb8AqwN2UV01Gt1mdmlrMOrda3FjgxIk4Cfg9cOULZ90bEKalLTjthm1lPSE3W403YEXFvROwtTh+gto9AKZywzaxnTMB62J8A7m4VDnCvpA3FblxtuQ/bzHrGKJLxNEkDdef9EdE/fCJpHTCjyc8tj4g7ijLLgb3ATS3e4+yI2CrpLcBaSY9HxP0jBeWEbWY9YxRfOu4YqV85IuaP9MOSLgbOB85ttZlLRGwt/rtd0u3APGDEhO0uETPrCZ3qw5a0APgCsCgiXmhR5jBJhw8fAx8AHmtXtxO2mfWMDvVhXwccTq2bY6OkFQCSjpG0pijzVuC3kh4BHgTuiohftKvYXSJm1jM6MXEmIo5vcX0rsLA4fhI4ebR1p2zCe6ikByU9ImmzpC81KXOJpGeKvyYbJV062kDMzKqW+67pKS3sPcD7IuJ5SVOpNePvjogHGsrdEhGXlR+imVk5ujkZp0jZhDeA54vTqcUr79/azHrOZNjAIOlLR0lTJG0EtgNrI2J9k2IfkbRJ0m2SZpcapZlZCXLvEklK2BHxSkScQm2K5TxJJzYU+Tkwp5g7vw5Y2aweScskDTQMSDcz64ieSNjDihWnfg0saLj+bETsKU6/A5ze4uf7I6IvdaETM7MyTfqELWm6pCOL49cD84HHG8rMrDtdBGwpM0gzs/Hq1MSZKqWMEpkJrJQ0hVqCvzUi7pR0DTAQEauBz0paRG3e/HPAJVUFbGY2Vt2cjFOkjBLZBJza5PpVdcdXMvKar2ZmEy73USKe6WhmPWPSt7DNzCaDbu+fTuGEbWY9wwnbzCwTTthmZpnwl45mZhlwH7aZWUacsM3MMuGEbWaWCSdsM7NMOGGbmWVgMmxg4IRtZj3DLWwzs0w4YZuZZcIJ28wsA544Y2aWESdsM7NMeJSImVkm3MI2M8vAZOjDbrtr+jBJUyT9r6Q7m9w7RNItkgYlrZc0p8wgzczK0Ild0yV9WdImSRsl3SvpmBblLpb0h+J1cUrdyQkbuBzY0uLeJ4GdEXE88HXgK6Oo18ysIzqRsIGvRsRJEXEKcCdwVWMBSUcBVwNnAPOAqyW9uV3FSQlb0izgn4DvtiiyGFhZHN8GnCtJKXWbmXXKvn37kl7jERG7604PA5r9BfggsDYinouIncBaYEG7ulP7sP8b+E/g8Bb3jwWeLoLdK2kXcDSwo76QpGXAsuJ0D/BY4vtXaRoNcXbaqlWrJjyGwoTH4WfRdTFAd8TxrhLquIfa75LiUEkDdef9EdGf+kaS/gu4CNgFvLdJkVdzZmGouDaitglb0vnA9ojYIOmcVsWaXDvgr0rxC/cX9Q5ERF+7969aN8TRDTF0SxzdEEO3xNENMXRLHA3Jc0wiom0LNpWkdcCMJreWR8QdEbEcWC7pSuAyat0fr6miWYjt3jelhX02sEjSQuBQ4AhJP4yIf6srMwTMBoYkHQS8CXguoW4zs+xExPzEoj8C7uLAhD0EnFN3Pgv4dbvK2vZhR8SVETErIuYAS4H7GpI1wGpg+FvOC4oyeY+fMTMbA0lz604XAY83KXYP8AFJby6+bPxAcW1EYx6HLekaYCAiVgM3AD+QNEitZb00oYrk/qCKdUMc3RADdEcc3RADdEcc3RADdEcc3RBDqmslvQvYB/wJ+BSApD7gUxFxaUQ8J+nLwEPFz1wTEW17JeSGsJlZHkYzDtvMzCaQE7aZWSYqT9iSFkh6opi2fkWT+5VPa0+I4RJJzxRTSTdKurSCGG6UtF1S07HnqvlGEeMmSaeVHUNiHOdI2lX3LA6YpVVCDLMl/UrSFkmbJV3epEylzyMxhk48i0MlPSjpkSKOLzUpU+lnJDGGyj8jde/lZTBaSZ2qOZYXMAX4I/AO4GDgEeCEhjL/DqwojpcCt0xADJcA11X8LP4BOA14rMX9hcDd1MZnngmsn6A4zgHurPhZzAROK44PB37f5H+TSp9HYgydeBYC3lgcTwXWA2c2lKn6M5ISQ+Wfkbr3+g9qw+EOePZVP4tuf1Xdwp4HDEbEkxHxMnAztWns9aqe1p4SQ+Ui4n5GHpu+GPh+1DwAHClp5gTEUbmI2BYRDxfHf6O2Rk3jLK9Kn0diDJUrfr/ni9OpxatxJECln5HEGDrCy2CMrOqEnTL98jXT2qlN5Ty6wzEAfKT4p/dtkmaX+P6pxjRVtSJnFf88vlvSe6p8o+KftKdSa9XV69jzGCEG6MCzKLoANgLbqa0v0fJZVPQZSYkBOvMZGV4Go9WCHpU/i25WdcJOmX45pimaJcfwc2BORJwErGP/X/BOqvo5pHoYeHtEnAx8E/hZVW8k6Y3AT4DPxWsXzIEOPY82MXTkWUTEK1Fb2W0WME/SiY1hNvuxDsdQ+WdEdctgjFSsybWeGZtcdcIenrI+bBawtVUZVTOtvW0MEfFsROwpTr8DnF7i+6dKeVaVi4jdw/88jog1wFRJqQvmJJM0lVqivCkiftqkSOXPo10MnXoWde/3V2rTkxvXvKj6M9I2hg59RoaXwXiKWtfl+yT9sKFMx55FN6o6YT8EzJV0nKSDqX1JsLqhTNXT2tvG0NA3uojW635XaTVwUTE64kxgV0Rs63QQkmYM9wlKmkft/yPPlvweojY7dktEfK1FsUqfR0oMHXoW0yUdWRy/HpjPgVOZK/2MpMTQic9IeBmMtirdIixqS61eRm2O/BTgxojYrPFPay87hs9KWgTsLWK4pMwYACT9mNqog2mShqgtBjO1iHEFsIbayIhB4AXg42XHkBjHBcCnJe0FXgSWVvCBOBv4GPBo0W8K8EXgbXVxVP08UmLoxLOYCayUNIXaH4RbI+LOTn5GEmOo/DPSSoefRVfz1HQzs0x4pqOZWSacsM3MMuGEbWaWCSdsM7NMOGGbmWXCCdvMLBNO2GZmmfh/ZA49Jo/5kbEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's see what it does\n",
    "policy, v = policy_improvement(env)\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "def print_grid_policy(policy, symbols=[\"^\", \">\", \"v\", \"<\"]):\n",
    "    symbols = np.array(symbols)\n",
    "    for row in policy:\n",
    "        print(\"\".join(symbols[row]))\n",
    "\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print_grid_policy(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "print(v.reshape(env.shape))\n",
    "print(\"\")\n",
    "\n",
    "plot_gridworld_value(v.reshape(env.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "eee712b931eb830cb89792ef30675558",
     "grade": true,
     "grade_id": "cell-695dc14dbc6a8f95",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is not an empty cell. It is needed for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c926edcbca292c79812c5b27eab63108",
     "grade": false,
     "grade_id": "cell-ceb755afcff43612",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 3. Value Iteration (3 points)\n",
    "Now implement the value iteration algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "87f515e22f7ad0ea461271479dff3f5e",
     "grade": false,
     "grade_id": "cell-574fc5f6932fa4cc",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def value_iteration(env, theta=0.0001, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Value Iteration Algorithm.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V) of the optimal policy and the optimal value function.        \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    V = np.zeros(env.nS)\n",
    "    policy = np.zeros([env.nS, env.nA])\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in np.arange(env.nS):\n",
    "            v = V[s]\n",
    "            state_dyn = env.P[s]\n",
    "\n",
    "            q_values = []\n",
    "            for a in np.arange(env.nA):\n",
    "                current_dynamics = state_dyn[a]\n",
    "                v_value = sum(psrsa * (reward + discount_factor * V[succ_state]) for psrsa, succ_state, reward, _ in current_dynamics)\n",
    "                q_values.append(v_value)\n",
    "\n",
    "            Vs_new = np.max(q_values)\n",
    "            V[s] = Vs_new\n",
    "            delta = np.max([delta, np.absolute(v - Vs_new)])\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "                \n",
    "    policy, _, _ = update_policy(policy, env, V, discount_factor)\n",
    "        \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d103427f5b98a8957ad486243f98e64c",
     "grade": true,
     "grade_id": "cell-b82ed3adfeecc757",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Probability Distribution:\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "\n",
      "Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\n",
      "[[0 3 3 2]\n",
      " [0 0 0 2]\n",
      " [0 0 1 2]\n",
      " [0 1 1 0]]\n",
      "^<<v\n",
      "^^^v\n",
      "^^>v\n",
      "^>>^\n",
      "\n",
      "Value Function:\n",
      "[ 0. -1. -2. -3. -1. -2. -3. -2. -2. -3. -2. -1. -3. -2. -1.  0.]\n",
      "\n",
      "Reshaped Grid Value Function:\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Oh let's test again\n",
    "# Let's see what it does\n",
    "policy, v = value_iteration(env)\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print_grid_policy(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "print(v.reshape(env.shape))\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3add7d8b6101d0e3b6250b6bb064566c",
     "grade": false,
     "grade_id": "cell-ded21ac846e244a3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "What is the difference between value iteration and policy iteration? Which algorithm is most efficient (e.g. needs to perform the least *backup* operations)? Please answer *concisely* in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "078f713af4c6bf3af8fb31b8da772758",
     "grade": true,
     "grade_id": "cell-940a8d8e21f18f69",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Policy iteration consists of two parts, policy evaluation and policy improvement. In theory, the value function's convergence to $v_\\pi$ only occurs in the limit of several iterations of policy evaluation with the associated multiple sweeps of the state space. Value iterations shows that this is unnecessary overhead, as we do not need to wait for policy evaluation to converge. In fact, we can alternate between one state space sweep of policy evaluation and one execution of policy improvement without losing convergence guarantees. Both policy evaluation and policy improvement can then be combined into one update step that is based on the Bellman\n",
    "optimality equation. This makes value iteration considerably more efficient than policy iteration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "98ec4e85c09c116f6fe1658fa0451e33",
     "grade": false,
     "grade_id": "cell-7ab207a9f93cf4d3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 4. Monte Carlo Prediction (7 points)\n",
    "What is the difference between Dynamic Programming and Monte Carlo? When would you use the one or the other algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "dba98a584a2d9c97735f96547ac7442a",
     "grade": true,
     "grade_id": "cell-74a904ed87b8e2cc",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "The biggest difference between them is that Dynamic Programming requieres complete\n",
    "knowledge of the environment's dynamics, whereas Monte Carlo methods sample trajectories of experience ($s, a, r, s'$) from the environment with which a policy can be learned. Thus Monte Carlo methods can enter the environment without any prior knowledge, except knowing available actions. It is possible to use Dynamic Programming in case of a completly known environment of moderate size, where the computations stay tractable. When the dynamics of the environment are unknown, but we know we face an episodic tasks (terminal state after finite number of steps) we can use Monte Carlo based methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0d826feda7dc9cab51ad9db8ccbdfadf",
     "grade": false,
     "grade_id": "cell-5f0c1d608436b67b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "For the Monte Carlo Prediction we will look at the Blackjack game (Example 5.1 from the book), for which the `BlackjackEnv` is implemented in `blackjack.py`. Note that compared to the gridworld, the state is no longer a single integer, which is why we use a dictionary to represent the value function instead of a numpy array. By using `defaultdict`, each state gets a default value of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fb7a884505c5973aff2fe9998cc104e2",
     "grade": false,
     "grade_id": "cell-a342b69fcfdea5b2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from blackjack import BlackjackEnv\n",
    "env = BlackjackEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "26ae90f402b5de7d07f23e776a525c4b",
     "grade": false,
     "grade_id": "cell-7366692dee80c32c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "For the Monte Carlo algorithm, we need to *interact* with the environment. This means that we start an episode by using `env.reset` and send the environment actions via `env.step` to observe the reward and next observation (state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ef5fc8c121d400f2192646c9201e7769",
     "grade": false,
     "grade_id": "cell-85356add2643980e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# So let's have a look at what we can do in general with an environment...\n",
    "import gym\n",
    "?gym.Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0433c9161cddc2d9171c8e87b1e9b444",
     "grade": false,
     "grade_id": "cell-251b7b17c5d08a24",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# We can also look at the documentation/implementation of a method\n",
    "?env.step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "42aceb4ea182a1aacfb40c5f201d4b01",
     "grade": false,
     "grade_id": "cell-6decb2ab83c5bcec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "??BlackjackEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8fa896a9ea64567134c861511cfa6011",
     "grade": false,
     "grade_id": "cell-ae161126d3cb1b7b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "A very simple policy for Blackjack is to *stick* if we have 20 or 21 points and *hit* otherwise. We want to know how good this policy is. This policy is *deterministic* and therefore a function that maps an observation to a single action. Technically, we can implement this as a dictionary or as a function, where we use the latter. To get started, let's implement this simple policy for BlackJack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "cc1b192852416b81f96f0858f1389d67",
     "grade": false,
     "grade_id": "cell-9fdcb503df9cdb08",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# possible Env functions: step, reset, render, close, seed\n",
    "# observation: players current sum, dealer's showing card, player holds a usable ace\n",
    "\n",
    "def simple_policy(observation):\n",
    "    \"\"\"\n",
    "    A policy that sticks if the player score is >= 20 and hits otherwise.\n",
    "    \"\"\"\n",
    "    return not(observation[0] >= 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bf04382f012b102f2c5c360bb8da3241",
     "grade": true,
     "grade_id": "cell-99f02e2d9b338a5b",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 9, False)\n",
      "((23, 9, False), -1, True, {})\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "print(s)\n",
    "a = simple_policy(s)\n",
    "print(env.step(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b5e33f6499dd4040071dc62a3e8fccb2",
     "grade": false,
     "grade_id": "cell-0184f4c719afb98c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now implement either the MC prediction algorithm (either first visit or every visit). Hint: you can use `for i in tqdm(range(num_episodes))` to show a progress bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a90a42914d0f7ade801bee9a8bd04e19",
     "grade": true,
     "grade_id": "cell-b822e9d13cf1f65e",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def mc_prediction(policy, env, num_episodes, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Monte Carlo prediction algorithm. Calculates the value function\n",
    "    for a given policy using sampling.\n",
    "    \n",
    "    Args:\n",
    "        policy: A function that maps an observation to action probabilities.\n",
    "        env: OpenAI gym environment.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary that maps from state -> value.\n",
    "        The state is a tuple and the value is a float.\n",
    "    \"\"\"\n",
    "\n",
    "    # Keeps track of sum and count of returns for each state\n",
    "    # to calculate an average. We could use an array to save all\n",
    "    # returns (like in the book) but that's memory inefficient.\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(float)\n",
    "    \n",
    "    # The final value function\n",
    "    V = defaultdict(float)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    for i in tqdm(range(num_episodes)):\n",
    "        rewards = []\n",
    "        states = []\n",
    "        actions = []\n",
    "        s = env.reset()\n",
    "        while True: \n",
    "            states.append(s)\n",
    "\n",
    "            a = policy(s)\n",
    "            actions.append(a)\n",
    "\n",
    "            s, r, done, _ = env.step(a)\n",
    "            rewards.append(r)\n",
    "\n",
    "            if done: \n",
    "                break\n",
    "          \n",
    "        G = 0\n",
    "        timesteps = len(rewards) - 1\n",
    "\n",
    "        for t in range(timesteps, -1, -1):\n",
    "            reward, state = rewards[t], states[t]\n",
    "            G = discount_factor * G + reward\n",
    "            returns_sum[state] += G\n",
    "            returns_count[state] += 1\n",
    "    \n",
    "    for state in returns_sum:\n",
    "        V[state] = returns_sum[state]/returns_count[state]\n",
    "      \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1000/1000 [00:00<00:00, 14311.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'float'>, {(20, 2, False): 0.8235294117647058, (14, 2, False): -0.5384615384615384, (20, 10, True): 0.5333333333333333, (20, 10, False): 0.5, (16, 10, False): -0.68, (16, 10, True): -0.625, (19, 10, False): -0.6170212765957447, (14, 10, False): -0.5555555555555556, (18, 10, False): -0.7297297297297297, (21, 8, True): 0.6666666666666666, (17, 1, False): -0.8, (19, 3, False): -0.7333333333333333, (19, 7, False): -0.8333333333333334, (12, 1, False): -0.3333333333333333, (21, 6, True): 1.0, (19, 8, False): -0.7777777777777778, (13, 8, False): -0.3333333333333333, (15, 10, True): -0.25, (19, 1, False): -1.0, (20, 4, False): 0.5238095238095238, (20, 3, False): 0.6, (20, 9, False): 0.8333333333333334, (16, 3, False): -1.0, (12, 3, False): -0.5, (12, 3, True): -1.0, (18, 8, False): -1.0, (15, 8, False): 0.0, (17, 8, False): -1.0, (15, 10, False): -0.5789473684210527, (21, 10, False): 0.8846153846153846, (13, 10, False): -0.4864864864864865, (20, 1, False): 0.4117647058823529, (16, 1, False): -0.6923076923076923, (13, 5, False): -0.5, (19, 10, True): 0.5, (20, 6, False): 0.6923076923076923, (16, 7, False): -0.5555555555555556, (21, 7, False): 1.0, (17, 10, False): -0.8846153846153846, (18, 10, True): 0.14285714285714285, (13, 10, True): 0.0, (15, 3, False): -0.4, (20, 7, False): 0.6666666666666666, (12, 10, False): -0.6666666666666666, (14, 6, False): -0.875, (15, 2, False): -0.7142857142857143, (21, 6, False): 1.0, (18, 6, False): -0.6923076923076923, (16, 5, False): -0.875, (21, 9, False): 1.0, (16, 9, False): -0.2, (20, 8, False): 0.8333333333333334, (13, 1, False): -0.5555555555555556, (16, 2, False): -0.46153846153846156, (12, 2, False): -1.0, (18, 1, False): -0.625, (17, 3, False): -1.0, (14, 3, False): -1.0, (17, 3, True): 0.0, (14, 7, False): -0.8333333333333334, (15, 7, True): 0.0, (13, 2, False): -0.5, (21, 10, True): 0.8636363636363636, (18, 1, True): 0.0, (16, 9, True): 1.0, (12, 7, False): -1.0, (19, 7, True): -0.3333333333333333, (12, 4, False): -0.7142857142857143, (12, 6, False): -0.7272727272727273, (19, 9, False): -0.75, (15, 9, False): -0.6666666666666666, (13, 9, False): -0.75, (14, 4, False): -0.4, (15, 4, True): -1.0, (17, 6, False): -1.0, (12, 9, False): 0.1111111111111111, (19, 6, False): -0.8181818181818182, (17, 7, False): -1.0, (13, 7, False): -0.8, (20, 4, True): 1.0, (19, 2, False): -0.4166666666666667, (17, 2, False): -0.5, (16, 8, False): -0.5714285714285714, (14, 8, False): -0.2727272727272727, (12, 8, False): -0.875, (18, 8, True): -1.0, (17, 8, True): -1.0, (17, 9, False): -0.8571428571428571, (14, 9, False): -0.6666666666666666, (18, 4, False): -0.5384615384615384, (17, 4, False): -0.46153846153846156, (21, 4, True): 1.0, (17, 10, True): -0.375, (21, 8, False): 0.9, (20, 3, True): 0.0, (17, 5, False): -0.875, (18, 7, False): -0.2, (21, 4, False): 0.75, (15, 1, False): -0.2, (20, 5, False): 0.65, (18, 2, False): -0.4, (21, 5, False): 0.7, (14, 5, False): -0.16666666666666666, (16, 6, False): -1.0, (18, 9, False): -1.0, (13, 9, True): -1.0, (14, 10, True): 0.0, (19, 5, False): -0.25, (20, 2, True): 0.5, (13, 2, True): 0.0, (15, 5, False): -0.35714285714285715, (12, 5, False): -0.625, (13, 6, False): -0.25, (15, 7, False): -0.6666666666666666, (16, 7, True): -1.0, (16, 8, True): -1.0, (15, 6, False): -0.8333333333333334, (21, 2, False): 0.875, (15, 5, True): -0.5, (21, 1, False): 1.0, (18, 5, False): -0.7, (21, 5, True): 1.0, (16, 4, False): -0.2222222222222222, (14, 7, True): -1.0, (17, 7, True): -0.3333333333333333, (18, 9, True): -1.0, (17, 9, True): 0.0, (14, 9, True): -1.0, (13, 4, False): 0.0, (21, 3, False): 0.8571428571428571, (13, 1, True): 0.0, (19, 4, False): -0.8333333333333334, (15, 4, False): -0.5, (14, 5, True): 0.0, (14, 1, False): -1.0, (17, 2, True): 0.0, (15, 2, True): 0.5, (20, 8, True): 1.0, (20, 7, True): 1.0, (20, 5, True): 1.0, (19, 5, True): 1.0, (15, 1, True): 0.0, (12, 1, True): 0.0, (17, 1, True): 0.0, (21, 1, True): 1.0, (21, 9, True): 0.7777777777777778, (21, 2, True): 1.0, (20, 9, True): 1.0, (18, 5, True): -1.0, (16, 5, True): -1.0, (16, 4, True): 0.0, (20, 1, True): 0.0, (21, 3, True): 0.6, (21, 7, True): 1.0, (16, 2, True): -0.3333333333333333, (18, 3, False): -1.0, (15, 8, True): 1.0, (18, 4, True): -1.0, (17, 4, True): -1.0, (14, 8, True): -1.0, (12, 9, True): -1.0, (12, 2, True): -1.0, (19, 3, True): -1.0, (13, 3, True): -1.0, (19, 9, True): 0.0, (15, 9, True): 0.0, (17, 6, True): -1.0, (18, 7, True): 1.0, (14, 2, True): 1.0, (13, 3, False): -1.0, (12, 6, True): -1.0, (19, 4, True): -1.0, (13, 4, True): -1.0, (14, 3, True): 1.0, (19, 2, True): 0.0, (18, 2, True): 0.0})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "V = mc_prediction(simple_policy, env, num_episodes=1000)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "65fb9d8060a4c843e72474169396eb3e",
     "grade": false,
     "grade_id": "cell-9d32f907f180c088",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now make *4 plots* like Figure 5.1 in the book. You can either make 3D plots or heatmaps. Make sure that your results look similar to the results in the book. Give your plots appropriate titles, axis labels, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8a4eee824f088a6c13ee8c9296af4561",
     "grade": false,
     "grade_id": "cell-cbaf4d6a0e4c00fa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10000/10000 [00:00<00:00, 22347.46it/s]\n",
      " 34%|      | 168425/500000 [00:09<00:16, 19542.47it/s]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Let's run your code one time\n",
    "V_10k = mc_prediction(simple_policy, env, num_episodes=10000)\n",
    "V_500k = mc_prediction(simple_policy, env, num_episodes=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "84102d69cdad07cf0100d846346b65e6",
     "grade": true,
     "grade_id": "cell-ba046443478aa517",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "plt.rcParams[\"figure.figsize\"] = [9,5]\n",
    "\n",
    "from mpl_toolkits.mplot3d.axes3d import get_test_data\n",
    "# This import registers the 3D projection, but is otherwise unused.\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import\n",
    "from matplotlib import cm\n",
    "from sys import argv\n",
    "\n",
    "# observation: players current sum, dealer's showing card, player holds a usable ace\n",
    "def get_data_from_V(V):\n",
    "    # X_t (player sum), Y_t (dealer card), Z_t (return) => usable ace true\n",
    "    X_t, Y_t, Z_t = [], [], []\n",
    "\n",
    "    # X_f, Y_f, Z_f => usable ace false\n",
    "    X_f, Y_f, Z_f = [], [], []\n",
    "    for observation, appr_v in V.items():\n",
    "        play_sum, deal_card, use_ace = observation\n",
    "        \n",
    "        if use_ace:\n",
    "            X_t.append(play_sum)\n",
    "            Y_t.append(deal_card)\n",
    "            Z_t.append(appr_v)\n",
    "        else:\n",
    "            X_f.append(play_sum)\n",
    "            Y_f.append(deal_card)\n",
    "            Z_f.append(appr_v)\n",
    "        \n",
    "    return (np.asarray(X_t), np.asarray(Y_t), np.asarray(Z_t)), (np.asarray(X_f), np.asarray(Y_f), np.asarray(Z_f))\n",
    "\n",
    "\n",
    "use_ace_10k, non_use_ace_10k = get_data_from_V(V_10k)\n",
    "use_ace_500k, non_use_ace_500k = get_data_from_V(V_500k)\n",
    "\n",
    "# set up a figure twice as wide as it is tall\n",
    "fig = plt.figure(figsize=(20, 15), dpi=300)\n",
    "\n",
    "ax1 = fig.add_subplot(2, 2, 1, projection='3d')\n",
    "ax1.set_ylim3d(12, 21)\n",
    "ax1.set_xlim3d(0, 10)\n",
    "ax1.set_zlim3d(-1, 1)\n",
    "ax1.set_title(\"After 10,000 episodes\", fontsize=30)\n",
    "ax1.set_xlabel('Dealer showing', fontsize=22)\n",
    "ax1.set_ylabel('Player sum', fontsize=22)\n",
    "ax1.text(0, 10, 0.75, \"Usable \\n  ace\", fontsize=30)\n",
    "ax1.plot_trisurf(use_ace_10k[1], use_ace_10k[0], use_ace_10k[2], cmap=cm.plasma, linewidth=0.1)\n",
    "\n",
    "ax2 = fig.add_subplot(2, 2, 2, projection='3d')\n",
    "ax2.set_ylim3d(12, 21)\n",
    "ax2.set_xlim3d(0, 10)\n",
    "ax2.set_zlim3d(-1, 1)\n",
    "ax2.set_title(\"After 500,000 episodes\", fontsize=30)\n",
    "ax2.set_xlabel('Dealer showing', fontsize=22)\n",
    "ax2.set_ylabel('Player sum', fontsize=22)\n",
    "ax2.plot_trisurf(use_ace_500k[1], use_ace_500k[0], use_ace_500k[2], cmap=cm.plasma, linewidth=0.1)\n",
    "\n",
    "ax3 = fig.add_subplot(2, 2, 3, projection='3d')\n",
    "ax3.set_ylim3d(12, 21)\n",
    "ax3.set_xlim3d(0, 10)\n",
    "ax3.set_zlim3d(-1, 1)\n",
    "ax3.set_xlabel('Dealer showing', fontsize=22)\n",
    "ax3.set_ylabel('Player sum', fontsize=22)\n",
    "ax3.text(0, 10, 0.75, \"   No\\nUsable\\n  ace\", fontsize=30)\n",
    "ax3.plot_trisurf(non_use_ace_10k[1], non_use_ace_10k[0], non_use_ace_10k[2], cmap=cm.plasma, linewidth=0.1)\n",
    "\n",
    "ax4 = fig.add_subplot(2, 2, 4, projection='3d')\n",
    "ax4.set_ylim3d(12, 21)\n",
    "ax4.set_xlim3d(0, 10)\n",
    "ax4.set_zlim3d(-1, 1)\n",
    "ax4.set_xlabel('Dealer showing', fontsize=22)\n",
    "ax4.set_ylabel('Player sum', fontsize=22)\n",
    "ax4.plot_trisurf(non_use_ace_500k[1], non_use_ace_500k[0], non_use_ace_500k[2], cmap=cm.plasma, linewidth=0.1)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c1baa558731a1618d32691224e5b5780",
     "grade": false,
     "grade_id": "cell-a5cc039e3d648855",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 5. Monte Carlo control with $\\epsilon$-greedy policy (5 points)\n",
    "Now we have a method to evaluate state-values given a policy. Take a moment to think whether we can use the value function to find a better policy? Assuming we do not know the dynamics of the environment, why is this not possible?\n",
    "\n",
    "We want a policy that selects _actions_ with maximum value, e.g. is _greedy_ with respect to the _action-value_ (or Q-value) function $Q(s,a)$. We need to keep exploring, so with probability $\\epsilon$ we will take a random action. First, lets implement a function `make_epsilon_greedy_policy` that takes the Q-value function and returns an $\\epsilon$-greedy policy. The policy itself is a function that returns an action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f1469d08ccd4665219d42e03b709c690",
     "grade": true,
     "grade_id": "cell-78eff3f4ca0f0e09",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function and epsilon.\n",
    "    \n",
    "    Args:\n",
    "        Q: A dictionary that maps from state -> action-values.\n",
    "            Each value is a numpy array of length nA (see below)\n",
    "        epsilon: The probability to select a random action . float between 0 and 1.\n",
    "        nA: Number of actions in the environment.\n",
    "    \n",
    "    Returns:\n",
    "        A function that takes the observation as an argument and returns\n",
    "        an action according to the epsilon-greedy policy.\n",
    "    \n",
    "    \"\"\"\n",
    "    def policy_fn(observation):\n",
    "        if np.random.uniform() < epsilon:\n",
    "            return np.random.randint(nA)\n",
    "        else:\n",
    "            return np.argmax(Q[observation])\n",
    "        \n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "12a7f4ab3c649a1d768ce80d4573cf28",
     "grade": true,
     "grade_id": "cell-2fc0baa87f31ab98",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def mc_control_epsilon_greedy(env, num_episodes, discount_factor=1.0, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Monte Carlo Control using Epsilon-Greedy policies.\n",
    "    Finds an optimal epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        epsilon: Chance the sample a random action. Float betwen 0 and 1.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (Q, policy).\n",
    "        Q is a dictionary mapping state -> action values.\n",
    "        policy is a function that takes an observation as an argument and returns\n",
    "        action probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    # Again, keep track of counts for efficiency\n",
    "    # returns_sum, returns_count and Q are \n",
    "    # nested dictionaries that map state -> (action -> action-value).\n",
    "    # We could also use tuples (s, a) as keys in a 1d dictionary, but this\n",
    "    # way Q is in the format that works with make_epsilon_greedy_policy\n",
    "    \n",
    "    returns_sum = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    returns_count = defaultdict(lambda: np.zeros(env.action_space.n, dtype=int))\n",
    "    \n",
    "    # The final action-value function.\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "   \n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "    \n",
    "    for i in tqdm(range(num_episodes)):\n",
    "        observation = env.reset()\n",
    "        episode = []\n",
    "        # Generate an episode following policy\n",
    "        while True:\n",
    "            action = policy(observation)\n",
    "            observation_, reward, done, _ = env.step(action)\n",
    "            episode.append((observation, reward, action))  \n",
    "            observation = observation_\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        visited = {}\n",
    "        G = 0\n",
    "        # BUG we also save/get somehow the observations where player sum is below 12...\n",
    "        # \n",
    "        \n",
    "        for step in episode[::-1]:\n",
    "            observation, reward, action = step\n",
    "            G = discount_factor * G + reward\n",
    "            if (observation, action) not in visited:\n",
    "                returns_sum[observation][action] += G\n",
    "                returns_count[observation][action] += 1\n",
    "                visited[(observation, action)] = \"dummy\"\n",
    "                \n",
    "                action_return_sum = returns_sum[observation][action]\n",
    "                action_return_count = returns_count[observation][action]\n",
    "                Q[observation][action] = action_return_sum / action_return_count\n",
    "        \n",
    "        policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "    \n",
    "    return Q, policy\n",
    "\n",
    "# Test it quickly\n",
    "Q, policy = mc_control_epsilon_greedy(env, num_episodes=10000, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9ea50e12589ce430405fa2e92ee0c108",
     "grade": false,
     "grade_id": "cell-e6170d8979ca2a9c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "Q, policy = mc_control_epsilon_greedy(env, num_episodes=500000, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "94d3f0aecbe1b5ebfed94e7b1379f617",
     "grade": false,
     "grade_id": "cell-449e36eb98369942",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "How can you obtain the (V-)value function from the Q-value function? Plot the (V-)value function that is the result of 500K iterations. Additionally, visualize the greedy policy similar to Figure 5.2 in the book. Use a white square for hitting, black for sticking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ef932ff88d976ffefc0338822d7d2af9",
     "grade": true,
     "grade_id": "cell-7d797248a3b132f5",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Q, policy = mc_control_epsilon_greedy(env, num_episodes=1000000, epsilon=0.2)\n",
    "\n",
    "Z_t = np.zeros((21, 10))\n",
    "Z_f = np.zeros((21, 10))\n",
    "\n",
    "for observation, q_values in Q.items():\n",
    "    opt_action = np.argmax(q_values)\n",
    "    player_sum, dealer_show, use_ace = observation\n",
    "\n",
    "    if use_ace:\n",
    "        Z_t[player_sum-1, dealer_show-1] = not(opt_action)\n",
    "    else:\n",
    "        Z_f[player_sum-1, dealer_show-1] = not(opt_action)\n",
    "        \n",
    "\n",
    "V = defaultdict(float)\n",
    "for observation, q_values in Q.items():\n",
    "    V[observation] = np.max(q_values)\n",
    "\n",
    "use_ace, non_use_ace = get_data_from_V(V)\n",
    "\n",
    "# set up a figure twice as wide as it is tall\n",
    "fig = plt.figure(figsize=(15, 13), dpi=150)\n",
    "\n",
    "ax1 = fig.add_subplot(2, 2, 1)\n",
    "c = ax1.pcolormesh(Z_t[11:, :], cmap='gray')\n",
    "ax1.set_title(\"$\\pi_*$\", fontsize=30)\n",
    "\n",
    "ax2 = fig.add_subplot(2, 2, 2, projection='3d')\n",
    "ax2.set_ylim3d(12, 21)\n",
    "ax2.set_xlim3d(0, 10)\n",
    "ax2.set_zlim3d(-1, 1)\n",
    "ax2.set_title(\"$V_{*}$\", fontsize=30)\n",
    "ax2.set_xlabel('Dealer showing', fontsize=22)\n",
    "ax2.set_ylabel('Player sum', fontsize=22)\n",
    "ax2.text(0, 10, 0.75, \"Usable \\n  ace\", fontsize=30)\n",
    "ax2.plot_trisurf(use_ace[1], use_ace[0], use_ace[2], cmap=cm.plasma, linewidth=0.1)\n",
    "\n",
    "ax3 = fig.add_subplot(2, 2, 3)\n",
    "c = ax3.pcolormesh(Z_f[10:, :], cmap='gray')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "ax4 = fig.add_subplot(2, 2, 4, projection='3d')\n",
    "ax4.set_ylim3d(12, 21)\n",
    "ax4.set_xlim3d(0, 11)\n",
    "ax4.set_zlim3d(-1, 1)\n",
    "ax4.set_xlabel('Dealer showing', fontsize=22)\n",
    "ax4.set_ylabel('Player sum', fontsize=22)\n",
    "ax4.text(0, 10, 0.75, \"   No\\nUsable\\n  ace\", fontsize=30)\n",
    "ax4.plot_trisurf(non_use_ace[1], non_use_ace[0], non_use_ace[2], cmap=cm.plasma, linewidth=0.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rl2019]",
   "language": "python",
   "name": "conda-env-rl2019-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
